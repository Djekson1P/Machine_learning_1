{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод ближайших соседей\n",
    "\n",
    "Метод ближайших соседей (k Nearest Neighbors, или kNN) — тоже очень популярный метод классификации, также иногда используемый в задачах регрессии. Это, наравне с деревом решений, один из самых понятных подходов к классификации. На уровне интуиции суть метода такова: посмотри на соседей, какие преобладают, таков и ты. Формально основой метода является гипотеза компактности: если метрика расстояния между примерами введена достаточно удачно, то схожие примеры гораздо чаще лежат в одном классе, чем в разных.\n",
    "\n",
    "\n",
    "Согласно методу ближайших соседей, тестовый пример (зеленый шарик) будет отнесен к классу \"синие\", а не \"красные\".\n",
    "\n",
    "\n",
    "\n",
    "Например, если не знаешь, какой тип товара указать в объявлении для Bluetooth-гарнитуры, можешь найти 5 похожих гарнитур, и если 4 из них отнесены к категории \"Аксессуары\", и только один — к категории \"Техника\", то здравый смысл подскажет для своего объявления тоже указать категорию \"Аксессуары\".\n",
    "\n",
    "\n",
    "Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:\n",
    "\n",
    "\n",
    "Вычислить расстояние до каждого из объектов обучающей выборки\n",
    "Отобрать  объектов обучающей выборки, расстояние до которых минимально\n",
    "Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди  ближайших соседей\n",
    "\n",
    "Под задачу регрессии метод адаптируется довольно легко – на 3 шаге возвращается не метка, а число – среднее (или медианное) значение целевого признака среди соседей.\n",
    "\n",
    "\n",
    "Примечательное свойство такого подхода – его ленивость. Это значит, что вычисления начинаются только в момент классификации тестового примера, а заранее, только при наличии обучающих примеров, никакая модель не строится. В этом отличие, например, от ранее рассмотренного дерева решений, где сначала на основе обучающей выборки строится дерево, а потом относительно быстро происходит классификация тестовых примеров.\n",
    "\n",
    "\n",
    "Стоит отметить, что метод ближайших соседей – хорошо изученный подход (в машинном обучении, эконометрике и статистике больше известно, наверное, только про линейную регрессию). Для метода ближайших соседей существует немало важных теорем, утверждающих, что на \"бесконечных\" выборках это оптимальный метод классификации. Авторы классической книги \"The Elements of Statistical Learning\" считают kNN теоретически идеальным алгоритмом, применимость которого просто ограничена вычислительными возможностями и проклятием размерностей.\n",
    "\n",
    "\n",
    "Метод ближайших соседей в реальных задачах\n",
    "\n",
    "В чистом виде kNN может послужить хорошим стартом (baseline) в решении какой-либо задачи;\n",
    "В соревнованиях Kaggle kNN часто используется для построения мета-признаков (прогноз kNN подается на вход прочим моделям) или в стекинге/блендинге;\n",
    "Идея ближайшего соседа расширяется и на другие задачи, например, в рекомендательных системах простым начальным решением может быть рекомендация какого-то товара (или услуги), популярного среди ближайших соседей человека, которому хотим сделать рекомендацию;\n",
    "На практике для больших выборок часто пользуются приближенными методами поиска ближайших соседей. Вот лекция Артема Бабенко про эффективные алгоритмы поиска ближайших соседей среди миллиардов объектов в пространствах высокой размерности (поиск по картинкам). Также известны открытые библиотеки, в которых реализованы такие алгоритмы, спасибо компании Spotify за ее библиотеку Annoy.\n",
    "\n",
    "Качество классификации/регрессии методом ближайших соседей зависит от нескольких параметров:\n",
    "\n",
    "\n",
    "число соседей\n",
    "метрика расстояния между объектами (часто используются метрика Хэмминга, евклидово расстояние, косинусное расстояние и расстояние Минковского). Отметим, что при использовании большинства метрик значения признаков надо масштабировать. Условно говоря, чтобы признак \"Зарплата\" с диапазоном значений до 100 тысяч не вносил больший вклад в расстояние, чем \"Возраст\" со значениями до 100.\n",
    "веса соседей (соседи тестового примера могут входить с разными весами, например, чем дальше пример, тем с меньшим коэффициентом учитывается его \"голос\")\n",
    "\n",
    "Класс KNeighborsClassifier в Scikit-learn\n",
    "\n",
    "Основные параметры класса sklearn.neighbors.KNeighborsClassifier:\n",
    "\n",
    "\n",
    "weights: \"uniform\" (все веса равны), \"distance\" (вес обратно пропорционален расстоянию до тестового примера) или другая определенная пользователем функция\n",
    "algorithm (опционально): \"brute\", \"ball_tree\", \"KD_tree\", или \"auto\". В первом случае ближайшие соседи для каждого тестового примера считаются перебором обучающей выборки. Во втором и третьем — расстояние между примерами хранятся в дереве, что ускоряет нахождение ближайших соседей. В случае указания параметра \"auto\" подходящий способ нахождения соседей будет выбран автоматически на основе обучающей выборки.\n",
    "leaf_size (опционально): порог переключения на полный перебор в случае выбора BallTree или KDTree для нахождения соседей\n",
    "metric: \"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\" и другие\n",
    "\n",
    "Выбор параметров модели и кросс-валидация\n",
    "\n",
    "Главная задача обучаемых алгоритмов – их способность обобщаться, то есть хорошо работать на новых данных. Поскольку на новых данных мы сразу не можем проверить качество построенной модели (нам ведь надо для них сделать прогноз, то есть истинных значений целевого признака мы для них не знаем), то надо пожертвовать небольшой порцией данных, чтоб на ней проверить качество модели.\n",
    "\n",
    "\n",
    "Чаще всего это делается одним из 2 способов:\n",
    "\n",
    "\n",
    "отложенная выборка (held-out/hold-out set). При таком подходе мы оставляем какую-то долю обучающей выборки (как правило от 20% до 40%), обучаем модель на остальных данных (60-80% исходной выборки) и считаем некоторую метрику качества модели (например, самое простое – долю правильных ответов в задаче классификации) на отложенной выборке.\n",
    "кросс-валидация (cross-validation, на русский еще переводят как скользящий или перекрестный контроль). Тут самый частый случай – K-fold кросс-валидация\n",
    "\n",
    "\n",
    "Тут модель обучается  раз на разных () подвыборках исходной выборки (белый цвет), а проверяется на одной подвыборке (каждый раз на разной, оранжевый цвет).\n",
    "Получаются  оценок качества модели, которые обычно усредняются, выдавая среднюю оценку качества классификации/регрессии на кросс-валидации.\n",
    "\n",
    "\n",
    "Кросс-валидация дает лучшую по сравнению с отложенной выборкой оценку качества модели на новых данных. Но кросс-валидация вычислительно дорогостоящая, если данных много.\n",
    "\n",
    "\n",
    "Кросс-валидация – очень важная техника в машинном обучении (применяемая также в статистике и эконометрике), с ее помощью выбираются гиперпараметры моделей, сравниваются модели между собой, оценивается полезность новых признаков в задаче и т.д. Более подробно можно почитать, например, тут у Sebastian Raschka или в любом классическом учебнике по машинному (статистическому) обучению\n",
    "\n",
    "\n",
    "Примеры применения\n",
    "\n",
    "Деревья решений и метод ближайших соседей в задаче прогнозирования оттока клиентов телеком-оператора\n",
    "\n",
    "Считаем данные в DataFrame и проведем предобработку. Штаты пока сохраним в отдельный объект Series, но удалим из датафрейма. Первую модель будем обучать без штатов, потом посмотрим, помогают ли они."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Метод ближайших соседей\n",
    "\n",
    "Метод ближайших соседей (k Nearest Neighbors, или kNN) —   метод машинного обучения обычно используемый в задачаха классификации Суть метода сводится к следующему: на участке выборке выбирается преобладающий элемент и все остальные на априорном уровне являются этим же элементом. \n",
    "\n",
    "\n",
    "Согласно методу ближайших соседей, тестовый пример (зеленый шарик) будет отнесен к классу \"синие\", а не \"красные\".\n",
    "Возьем случайную выборку в которой будет 4 зеленых доски и 1 красная. Поскольку красных большинство, то 1-у красную тоже посчитают как красную.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:\n",
    "\n",
    "Вычислить расстояние до каждого из объектов обучающей выборки Отобрать объектов обучающей выборки, расстояние до которых минимально Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди ближайших соседей\n",
    "\n",
    "Под задачу регрессии метод адаптируется довольно легко – на третьем шаге возвращается не метка, а число – среднее (или медианное) значение целевого признака среди соседей.\n",
    "\n",
    "\n",
    "Примечательное свойство такого подхода – его ленивость. Это значит, что вычисления начинаются только в момент классификации тестового примера, а заранее, только при наличии обучающих примеров, никакая модель не строится. В этом отличие, например, от ранее рассмотренного дерева решений, где сначала на основе обучающей выборки строится дерево, а потом относительно быстро происходит классификация тестовых примеров.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Число соседей метрика расстояния между объектами (часто используются метрика Хэмминга, евклидово расстояние, косинусное расстояние и расстояние Минковского). Отметим, что при использовании большинства метрик значения признаков надо масштабировать. Условно говоря, чтобы признак \"Зарплата\" с диапазоном значений до 100 тысяч не вносил больший вклад в расстояние, чем \"Возраст\" со значениями до 100. веса соседей (соседи тестового примера могут входить с разными весами, например, чем дальше пример, тем с меньшим коэффициентом учитывается его \"голос\")\n",
    "\n",
    "Класс KNeighborsClassifier в Scikit-learn\n",
    "\n",
    "Основные параметры класса sklearn.neighbors.KNeighborsClassifier:\n",
    "\n",
    "weights: \"uniform\" (все веса равны), \"distance\" (вес обратно пропорционален расстоянию до тестового примера) или другая определенная пользователем функция algorithm (опционально): \"brute\", \"ball_tree\", \"KD_tree\", или \"auto\". В первом случае ближайшие соседи для каждого тестового примера считаются перебором обучающей выборки. Во втором и третьем — расстояние между примерами хранятся в дереве, что ускоряет нахождение ближайших соседей. В случае указания параметра \"auto\" подходящий способ нахождения соседей будет выбран автоматически на основе обучающей выборки. leaf_size (опционально): порог переключения на полный перебор в случае выбора BallTree или KDTree для нахождения соседей metric: \"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\" и другие\n",
    "\n",
    "Выбор параметров модели и кросс-валидация\n",
    "\n",
    "Главная задача обучаемых алгоритмов – их способность обобщаться, то есть хорошо работать на новых данных. Поскольку на новых данных мы сразу не можем проверить качество построенной модели (нам ведь надо для них сделать прогноз, то есть истинных значений целевого признака мы для них не знаем), то надо пожертвовать небольшой порцией данных, чтоб на ней проверить качество модели.\n",
    "\n",
    "Чаще всего это делается одним из 2 способов:\n",
    "\n",
    "отложенная выборка (held-out/hold-out set). При таком подходе мы оставляем какую-то долю обучающей выборки (как правило от 20% до 40%), обучаем модель на остальных данных (60-80% исходной выборки) и считаем некоторую метрику качества модели (например, самое простое – долю правильных ответов в задаче классификации) на отложенной выборке. кросс-валидация (cross-validation, на русский еще переводят как скользящий или перекрестный контроль). Тут самый частый случай – K-fold кросс-валидация\n",
    "\n",
    "Тут модель обучается раз на разных () подвыборках исходной выборки (белый цвет), а проверяется на одной подвыборке (каждый раз на разной, оранжевый цвет). Получаются оценок качества модели, которые обычно усредняются, выдавая среднюю оценку качества классификации/регрессии на кросс-валидации.\n",
    "\n",
    "Кросс-валидация дает лучшую по сравнению с отложенной выборкой оценку качества модели на новых данных. Но кросс-валидация вычислительно дорогостоящая, если данных много.\n",
    "\n",
    "\n",
    "Примеры применения\n",
    "\n",
    "Деревья решений и метод ближайших соседей в задаче прогнозирования оттока клиентов телеком-оператора\n",
    "\n",
    "Считаем данные в DataFrame и проведем предобработку. Штаты пока сохраним в отдельный объект Series, но удалим из датафрейма. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подключим библиотеки\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Подружаем изначальный набора данных\n",
    "data = pd.read_csv(r'C:\\Тест\\Python_Обучение\\data\\titanic.csv')\n",
    "\n",
    "# Выбираем обучающий набор, удаляя не нужные столбцы\n",
    "x = data.drop([\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "#преобразуем строковые значения в числовые. Для проведения дальльнейшего анализа.\n",
    "x = pd.get_dummies(x)\n",
    "\n",
    "# Все пропуски в обучаеющем наборе заполним медианным значением\n",
    "x = x.fillna({'Age': x.Age.median()})\n",
    "\n",
    "# Целевой набор. В данном случае остался ли жив человек.\n",
    "y = data.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Разбиваем на тестовые и тренировочные признаки и ответы в соотношении 67 к 33.\n",
    "# random_state - задает зерно случайности при выборке строк. \n",
    "# Если его не задать, то при каждом запуске в тест и train будут попадать разные строки.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state = 42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score_train = 0.7019230769230769\n",
      "Score_test =  0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Задаем параметры для обучения- ближайших соседей из библиотеки sklearn.\n",
    "knn = KNeighborsClassifier(n_neighbors=7,\n",
    "                          #max_depth = 3,\n",
    "                          )\n",
    "\n",
    "# Обучаем соседей\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Находим число правильных ответов на обучающей выборке\n",
    "Score_train = knn.score(X_train, y_train) \n",
    "print(\"Score_train =\", Score_train)\n",
    "\n",
    "# на тестовой выборке всего 78% правильных результатов\n",
    "Score_test = knn.score(X_test, y_test) \n",
    "print(\"Score_test = \", Score_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
