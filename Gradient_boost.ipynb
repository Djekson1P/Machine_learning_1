{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бустинг это ансамбль алгоритмов предназначенный для поиска наиболее выгодного решения задачи.\n",
    "\n",
    "Суть Бустинга, равно как и других ансамблей алгоритмов, состоит в том, чтобы из нескольких слабых моделей собрать одну сильную. Общая идея алгоритмов Бустинга - последовательно применять предикторы так, чтобы каждая последующая модель минимизировала ошибку предыдущей.\n",
    "\n",
    "Общий процес описания работы бустинговый алгоритмов можно свести к следующему. Веса ошибочно сработанных моделей увеличиваются, и в следущем проходе алгоритм больше внимания уделяет уже конкретной области.\n",
    "\n",
    "Такая техника последовательного обучения напоминает градиентный спуск, только вместо изменения параметров одного предиктора для минимизации функции потерь AdaBoost добавляет модели в ансамбль, постепенно улучшая его. Большим недостатком этого алгоритма можно считать то, что его нельзя распараллелить, поскольку каждый из предикторов может быть обучен лишь только после окончания обучения предыдущего.\n",
    "\n",
    "\n",
    "AdaBoost работает по следующей схеме:\n",
    "1. Изначально всем точкам присвоены равные веса\n",
    "2. Модель строится на подвыборке данных.\n",
    "3. По этой модели получаются предсказания для всех данных.\n",
    "4. По предсказаниям и истинным значениям вычисляются ошибки.\n",
    "5. В построении следующей модели наибольшие веса присваиваются точкам данных, на предсказании которых алгоритм ошибся.\n",
    "6. Веса могут быть определены по величине ошибки. А именно, чем больше ошибка, тем больше вес.\n",
    "7. Этот процесс повторяется, пока функция ошибки не перестанет меняться или пока не будет достигнуто максимальное число предикторов.\n",
    "\n",
    "Гиперпараметры:\n",
    "\n",
    "base_estimator : определяет базовый алгоритм.\n",
    "n_estimator : определяет количество базовых алгоритмов; по умолчанию - 10, но для улучшения реализации их можно увеличить.\n",
    "learning_rate : коэффициент скорости обучения - параметр, отвечающий за то, насколько изменяются веса.\n",
    "max_depth : максимальная глубина каждой модели.\n",
    "n_jobs : параметр, показывающий, сколько ядер процессора можно использовать для процесса обучения. “-1” значит, что ограничения нет.\n",
    "random_state : делает ответ модели повторимым. Модель всегда будет давать один и тот же ответ на одних и тех же данных и параметрах при совпадении значения этого параметра.\n",
    "\n",
    "Градиентный Бустинг\n",
    "Другой  очень популярный бустинговый алгоритм, принцип работы которого очень похож на рассмотренный только что AdaBoost. Градиентный Бустинг работает последовательно добавляя к прошлым моделям новые так, чтобы исправлялись ошибки, допущенные предыдущими предикторами.\n",
    "Градиентный Бустинг отличается от Адаптивного тем, что, в отличие от AdaBoost, изменяющего веса при каждой итерации, Градиентный пытается обучать новые модели по остаточной ошибке прошлых (двигаясь к минимуму функции потерь).\n",
    "Для лучшего понимания этого алгоритма важно разобраться в работе градиентного спуска.\n",
    "\n",
    "\n",
    "Приведем шаги реализации Градиентного Бустинга:\n",
    "1. Модель строится по подборке данных.\n",
    "2. Эта модель делает предсказания для всего набора данных.\n",
    "3. По предсказаниям и истинным значениям вычисляются ошибки.\n",
    "4. Новая модель строится с учетом ошибок как целевых переменных. При этом мы стремимся найти лучшее разделение для минимизации ошибки.\n",
    "5. Предсказания, сделанные с помощью этой новой модели, сочетаются с предсказаниями предыдущих.\n",
    "6. Снова вычисляются ошибки с использованием этих предсказанных значений и истинных значений.\n",
    "7. Этот процесс повторяется, пока функция ошибки не перестанет меняться или пока не будет достигнуто максимальное число предикторов.\n",
    "\n",
    "Гиперпараметры:\n",
    "\n",
    "min_samples_split : минимальное число точек, необходимое для разделение. Полезно, чтобы избегать переобучение.\n",
    "min_samples_leaf : минимальное количество элементов в листе или узле дерева. Меньшие значения следует выбирать для несбалансированных выборок.\n",
    "min_weight_fraction_leaf : похож на предыдущий, только вместо количества задает долю от общего числа элементов.\n",
    "max_depth : максимальная глубина дерева. Используется для борьбы с переобучением.\n",
    "max_lead_nodes : Максимальное число конечных листьев у дерева. Если задан этот гиперпараметр, то предыдущий игнорируется.\n",
    "max_features : количество признаков, учитываемых алгоритмом при поиске лучше разделения.\n",
    "\n",
    "XGBoost\n",
    "Экстремальный Градиентный Бустинг (XGBoost - Extreme Gradient Boosting) - это продвинутая реализация Градиентного Бустинга. Этот алгоритм обладает высокой предсказательной способностью и в десять раз быстрее любых других методов градиентного бустинга. Кроме того, включает в себя различные регуляризации, что уменьшает переобучение и улучшает общую производительность.\n",
    "\n",
    "Преимущества\n",
    "1. Осуществляет регуляризацию, что помогает бороться с переобучением.\n",
    "2. Возможно распараллеливание, что делает его намного быстрее Градиентного Бустинга.\n",
    "3. Позволяет пользователю определить собственные цели оптимизации и критерии оценки, добавляя измерения в модель.\n",
    "4. Имеет встроенную подпрограмму для обработки пропущенных значений.\n",
    "5. Находит разделения до заданной максимальной глубины, а затем начинает обрезать дерево и удалять разделения, после которых нет положительных выводов.\n",
    "6. Позволяет производить кросс-валидацию на каждой итерации бустинга и следовательно облегчает вычисление оптимального числа итераций бустинга.\n",
    "\n",
    "Light GB\n",
    "Для особенно больших наборов данных Легкий Градиентный Бустинг лучше остальных, так как требует меньше времени.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждое дерево должно уменьшать ошибку (компенсировать ошибку предыдущего шага). \n",
    "Для этого вводятся 2 связанных параметра: n_estimators и learning_rate.\n",
    "Чем меньше learning_rate (шаг отрицательного прироста ошибки), тем больше n_estimators (небольших деревьев) нужно.\n",
    "У деревьев задается небольшая высота, обычно 2 - 8.\n",
    "An = An-1 + learning_rate * Bb\n",
    "\n",
    "Ошибка = (предсказанный ответ - реальный ответ)**2\n",
    "Направление ошибки (производная) = 2 * (предсказанный ответ - реальный ответ)\n",
    "Если классификация 0 или 1, то лучше использовать сигмойдальную активационную функцию.\n",
    "Описание активационных функций и градиентного спуска для уменьшения ошибки описано тут: Введение в нейронные сети\n",
    "\n",
    "Преимущество перед random forest - что обучение направленное (не случайное), и хороший результат может быть \n",
    "достигнут на меньшем числе деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подргужаем библиотеки\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Подружаем изначальный набора данных\n",
    "data = pd.read_csv(r'C:\\Тест\\Python_Обучение\\data\\titanic.csv')\n",
    "\n",
    "# Выбираем обучающий набор, удаляя не нужные столбцы\n",
    "x = data.drop([\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "#преобразуем строковые значения в числовые. Для проведения дальльнейшего анализа.\n",
    "x = pd.get_dummies(x)\n",
    "\n",
    "# Все пропуски в обучаеющем наборе заполним медианным значением\n",
    "x = x.fillna({'Age': x.Age.median()})\n",
    "\n",
    "# Целевой набор. В данном случае остался ли жив человек.\n",
    "y = data.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Разбиваем на тестовые и тренировочные признаки и ответы в соотношении 67 к 33.\n",
    "# random_state - задает зерно случайности при выборке строк. \n",
    "# Если его не задать, то при каждом запуске в тест и train будут попадать разные строки.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state = 42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Градиентный бустинг\n",
    "\n",
    "parameters = {'n_estimators': [10, 15, 20, 30], \n",
    "              'learning_rate': [0.1,0.5,1], \n",
    "              'max_depth': [3,5,7,10],\n",
    "              'min_samples_split': [2,5,7,10,15,20], \n",
    "              'min_samples_leaf': [1,2,5,7,10]\n",
    "             }\n",
    "GB = GradientBoostingClassifier()\n",
    "grid_search_cv_clf = GridSearchCV(GB,\n",
    "                                  parameters,\n",
    "                                  cv=5,\n",
    "                                  n_jobs=-1\n",
    "                                 )\n",
    "\n",
    "grid_search_cv_clf.fit(X_train, y_train)\n",
    "best_clf = grid_search_cv_clf.best_estimator_\n",
    "grid_search_cv_clf.best_params_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.85%\n"
     ]
    }
   ],
   "source": [
    "# Точность для Градиентного бустинга составит при этом:\n",
    "accuracy = best_clf.score(X_test, y_test) \n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# АдаБуст\n",
    "\n",
    "# Зададим модель на \"минималках\"\n",
    "AB = AdaBoostClassifier()\n",
    "\n",
    "\n",
    "AB.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_clf = grid_search_cv_clf.best_estimator_\n",
    "grid_search_cv_clf.best_params_ \n",
    "grid_search_cv_clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.23%\n"
     ]
    }
   ],
   "source": [
    "# Точность для алгоритма АдаБуст на \"минималках\" составит при этом:\n",
    "accuracy = AB.score(X_test, y_test) \n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень презентабельная точность. Не так ли? Давайте попробуем совсем немного улучшить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# АдаБуст с улучшеним\n",
    "\n",
    "# Зададим модель с улучшением:\n",
    "AB = AdaBoostClassifier(algorithm='SAMME')\n",
    "\n",
    "AB.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_clf = grid_search_cv_clf.best_estimator_\n",
    "grid_search_cv_clf.best_params_ \n",
    "grid_search_cv_clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.92%\n"
     ]
    }
   ],
   "source": [
    "# Точность для алгоритма АдаБуст на \"минималках\" составит при этом:\n",
    "accuracy = AB.score(X_test, y_test) \n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается более интересный показатель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGboost\n",
    "\n",
    "# Создаем модель, для тестового примера параметры задавать не будем\n",
    "xgb = xgboost.XGBClassifier()\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.92%\n"
     ]
    }
   ],
   "source": [
    "# Оценим точность модели\n",
    "\n",
    "# Сделаем прогноз на тестовой выборке:\n",
    "y_pred = xgb.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Расчитаем точность:\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
