{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "RF (random forest) — это множество деревьев решений. При решении задачии регрессии ответы усредняются, при решении задачи\n",
    "классификаци приоритет отдаются максимамально найденному параметру.\n",
    "\n",
    "Суть случайного леса сводится к следующему: выбирается подвыборка обучающей выборки размера samplesize  – по ней строится дерево\n",
    "(для каждого дерева — своя подвыборка). Для построения каждого ответлвения в дереве просматриваем max_features случайных\n",
    "признаков (для каждого нового ответлвения — свои случайные признаки). Выбираем наилучшие признак и расщепление по нему\n",
    "(по заранее заданному критерию). Дерево строится, как правило, до исчерпания выборки (пока в листьях не останутся представители\n",
    "только одного класса), но в современных реализациях есть параметры, которые ограничивают высоту дерева, число объектов в \n",
    "листьях и число объектов в подвыборке, при котором проводится расщепление. Такая схема построения соответствует главному\n",
    "принципу ансамблирования (построению алгоритма машинного обучения на базе нескольких, в данном случае решающих деревьев): \n",
    "базовые алгоритмы должны быть хорошими и разнообразными (поэтому каждое дерево строится на своей обучающей выборке и \n",
    "при выборе расщеплений есть элемент случайности).\n",
    "\n",
    "Основные параметры:\n",
    "\n",
    "n_estimators -Число деревьев.\n",
    "Чем больше деревьев, тем лучше качество, но время настройки и работы RF также пропорционально увеличиваются. Обратите внимание, что часто при увеличении n_estimators качество на обучающей выборке повышается (может даже доходить до 100%), а качество на тесте выходит на асимптоту (можно прикинуть, скольких деревьев Вам достаточно).\n",
    "\n",
    "max_features - Число признаков для выбора расщепления\n",
    "График качества на тесте от значения этого праметра унимодальный, на обучении он строго возрастает. При увеличении max_features увеличивается время построения леса, а деревья становятся «более однообразными». По умолчанию он равен sqrt(n) в задачах классификации и n/3 в задачах регрессии. Это ключевой параметр. Его настраивают в первую очередь (при достаточном числе деревьев в лесе).\n",
    "\n",
    "min_samples_split - Минимальное число объектов, при котором выполняется расщепление\n",
    "Допускается оставлять этот параметр по умолчанию.\n",
    "\n",
    "max_depth - Максимальная глубина деревьев\n",
    "Ясно, что чем меньше глубина, тем быстрее строится и работает RF. При увеличении глубины резко возрастает качество на обучении, но и на контроле оно, как правило, увеличивается. Рекомендуется использовать максимальную глубину (кроме случаев, когда объектов слишком много и получаются очень глубокие деревья, построение которых занимает значительное время). При использовании неглубоких деревьев изменение параметров, связанных с ограничением числа объектов в листе и для деления, не приводит к значимому эффекту (листья и так получаются «большими»). Неглубокие деревья рекомендуют использовать в задачах с большим числом шумовых объектов (выбросов).\n",
    "\n",
    "criterion - Критерий расщепления\n",
    "По смыслу это очень важный параметр, но по факту здесь нет вариантов выбора. В библиотеке sklearn для регрессии реализованы два критерия: “mse” и “mae”, соответствуют функциям ошибки, которые они минимизируют. В большинстве задач используется mse.  Для классификации реализованы критерии “gini” и “entropy”, которые соответствуют классическим критериям расщепления: Джини и энтропийному. \n",
    "\n",
    "В sklearn-реализации случайного леса нет параметра samplesize, который регламентирует, из скольких объектов делать подвыборку для построения каждого дерева. Зачастую он и не нужен, поскольку оптимально выбирать из всей выборки. Также рекомендуется выбирать подвыборку с возвращением: bootstrap=True ( вообщем и целом это и есть пресловутый бэггинг — bootstrap aggregating).\n",
    "\n",
    "По умолчанию в sklearn-овских методах n_jobs=1, т.е. случайный лес строится на одном процессоре. Если Вы хотите существенно ускорить построение, используйте n_jobs=-1 (строить на максимально возможном числе процессоров). Для построения воспроизводимых экспериментов используйте предустановку генератора псевдослучайных чисел: random_state.\n",
    "\n",
    "Помимо прочего при   построении леса random forest параллельно может вычисляться т.н. oob-оценка качества алгоритма (которая очень точная и получается не в ущерб разделения на обучение/тест), oob-ответы алгоритмы (ответы, которые выдавал бы алгоритм на обучающей выборке, если бы «обучался не на ней»), оцениваются важности признаков. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подргужаем библиотеки\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Подружаем изначальный набора данных\n",
    "data = pd.read_csv(r'C:\\Тест\\Python_Обучение\\data\\titanic.csv')\n",
    "\n",
    "# Выбираем обучающий набор, удаляя не нужные столбцы\n",
    "x = data.drop([\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "#преобразуем строковые значения в числовые. Для проведения дальльнейшего анализа.\n",
    "x = pd.get_dummies(x)\n",
    "\n",
    "# Все пропуски в обучаеющем наборе заполним медианным значением\n",
    "x = x.fillna({'Age': x.Age.median()})\n",
    "\n",
    "# Целевой набор. В данном случае остался ли жив человек.\n",
    "y = data.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем на тестовые и тренировочные признаки и ответы в соотношении 67 к 33.\n",
    "# random_state - задает зерно случайности при выборке строк. \n",
    "# Если его не задать, то при каждом запуске в тест и train будут попадать разные строки.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest - создание нескольких деревьев, но каждому дереву выделена часть признаков (колонок) и часть данных (строк)\n",
    "Всем деревья переданы одинаковые параметры обучения (глубина, размер сплита и пр.). Результатом является усредненная \n",
    "вероятность деревьев, отработавших по своей части столбцов и строк.\n",
    "Причем, чем больше деревьев - тем лучше.\n",
    "\n",
    "Случайный лес решает сразу 2 проблемы обучения деревьев:\n",
    "* множеств невысоких деревьев - решает проблему переобучения\n",
    "* перебор порядка колонок - проблему неоптимальности жадного алгоритма, т.к. выбирает порядок фич почти случайно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6153846153846154"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Зададим параметры модели\n",
    "RF = RandomForestClassifier(n_estimators=10,\n",
    "                            max_depth = 3,\n",
    "                            \n",
    "                           )\n",
    "# Обучим модель\n",
    "RF.fit(X_train, y_train) \n",
    "\n",
    "# Предскажем значения на тестовой выборке\n",
    "RF.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63 % точности, совсем не то чего хотелось бы получить от алгоритма машинного обучения. Попробуем добавить параметров и использовать метод GridSearchCV. Который подберет наилучшие параметры при обучении.                                                                                                                                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8269230769230769"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "#n_estimators - число деревьев\n",
    "parameters = {'n_estimators': [10, 20, 30], 'max_depth': [2,5,7,10]}\n",
    "grid_search_cv_clf = GridSearchCV(clf_rf, parameters, cv=5)\n",
    "grid_search_cv_clf.fit(X_train, y_train)\n",
    "\n",
    "grid_search_cv_clf.best_params_ # {'max_depth': 5, 'n_estimators': 30}\n",
    "best_clf = grid_search_cv_clf.best_estimator_\n",
    "grid_search_cv_clf.best_params_\n",
    "best_clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "82% значительно более хороший результат нежели чем 62%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7692307692307693"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestClassifier()\n",
    "\n",
    "#n_estimators - число деревьев\n",
    "parameters = {'n_estimators': [10, 20, 30], \n",
    "              'max_depth': [2,5,7,10],\n",
    "             }\n",
    "grid_search_RF = GridSearchCV(RF,\n",
    "                              parameters,\n",
    "                              cv=5)\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "\n",
    "grid_search_RF.best_params_ \n",
    "best_RF = grid_search_RF.best_estimator_\n",
    "grid_search_RF.best_params_\n",
    "best_RF.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "76 % уже лучше. Теперь попробуем еще один метод подбора наиболее оптимальных параметров RandomizedSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7692307692307693"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestClassifier()\n",
    "\n",
    "#n_estimators - число деревьев\n",
    "parameters = {'n_estimators': [10, 20, 30], \n",
    "              'max_depth': [2,5,7,10],\n",
    "             }\n",
    "grid_search_RF = RandomizedSearchCV(RF,\n",
    "                              parameters,\n",
    "                              cv=5)\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "\n",
    "grid_search_RF.best_params_ # {'max_depth': 5, 'n_estimators': 30}\n",
    "best_RF = grid_search_RF.best_estimator_\n",
    "grid_search_RF.best_params_\n",
    "best_RF.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим результат идентичен. Но у данного метода есть одно важное приемущество. Он работает несколько более быстрее чем \n",
    "GridSearchCV. На больших выборках это особенно заметно."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
